[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Crash Course in Matrix Algebra",
    "section": "",
    "text": "Welcome\nDue to the multivariate character of many econometric topics, matrix algebra is a commonly used tool in modern econometrics. It provides a powerful and efficient framework for representing and manipulating systems of linear equations. This short lecture note series provides a brief introduction to the most relevant matrix algebra concepts for econometricians and their implementation in R.\nTo learn R or refresh your skills, please check out my tutorial Getting Started With R."
  },
  {
    "objectID": "index.html#accompanying-r-scripts",
    "href": "index.html#accompanying-r-scripts",
    "title": "Crash Course in Matrix Algebra",
    "section": "Accompanying R scripts",
    "text": "Accompanying R scripts\nAll R codes of the different sections can be found here:\n\nmatrix-sec1.R\nmatrix-sec2.R\nmatrix-sec3.R\nmatrix-sec4.R"
  },
  {
    "objectID": "index.html#comments",
    "href": "index.html#comments",
    "title": "Crash Course in Matrix Algebra",
    "section": "Comments",
    "text": "Comments\nFeedback is welcome. If you notice any typos or issues, please report them on GitHub or email me at sven.otto@uni-koeln.de."
  },
  {
    "objectID": "01_basics.html#scalar-vector-and-matrix",
    "href": "01_basics.html#scalar-vector-and-matrix",
    "title": "\n1  Basic definitions\n",
    "section": "\n1.1 Scalar, vector, and matrix",
    "text": "1.1 Scalar, vector, and matrix\nA scalar \\(a\\) is a single real number. We write \\(a \\in \\mathbb R\\).\nA vector \\(\\boldsymbol a\\) of length \\(k\\) is a \\(k \\times 1\\) list of real numbers \\[\n\\boldsymbol a = \\begin{pmatrix} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_k \\end{pmatrix}.\n\\] By default, when we refer to a vector, we always mean a column vector. We write \\(\\boldsymbol a \\in \\mathbb R^k\\). The value \\(a_i\\) is called \\(i\\)-th entry or \\(i\\)-th component of \\(\\boldsymbol a\\). A scalar is a vector of length 1. A row vector of length \\(k\\) is written as \\(\\boldsymbol b = (b_1, \\ldots, b_k)\\).\nA matrix \\(\\boldsymbol A\\) of order \\(k \\times m\\) is a rectangular array of real numbers \\[\\boldsymbol A =\\begin{pmatrix}\n             a_{11} & a_{12} & \\cdots & a_{1m}\\\\\n             a_{21} & a_{22} & \\cdots & a_{2m}\\\\\n             \\vdots & \\vdots &       & \\vdots\\\\\n             a_{k1} & a_{k2} & \\cdots & a_{km}\n\\end{pmatrix}\\] with \\(k\\) rows and \\(m\\) columns. We write \\(\\boldsymbol A \\in \\mathbb R^{k \\times m}\\). The value \\(a_{ij}\\) is called \\((i,j)\\)-th entry or \\((i,j)\\)-th component of \\(\\boldsymbol A\\). We also use the notation \\((\\boldsymbol A)_{i,j}\\) to denote the \\((i,j)\\)-th entry. A vector of length \\(k\\) is a \\(k \\times 1\\) matrix. A row vector of length \\(k\\) is a \\(1 \\times k\\) matrix. A scalar is a matrix of order \\(1 \\times 1\\).\nWe may describe a matrix \\(\\boldsymbol A\\) by its column or row vectors as \\[\n\\boldsymbol A = \\begin{pmatrix} \\boldsymbol a_1 & \\boldsymbol a_2 & \\ldots & \\boldsymbol a_m \\end{pmatrix}\n= \\begin{pmatrix} \\boldsymbol \\alpha_1 \\\\ \\vdots \\\\ \\boldsymbol \\alpha_k \\end{pmatrix},\n\\] where \\[\n\\boldsymbol a_i = \\begin{pmatrix} a_{1i} \\\\ \\vdots \\\\ a_{ki} \\end{pmatrix}\n\\] is the \\(i\\)-th column of \\(\\boldsymbol A\\) and \\(\\boldsymbol \\alpha_i = (a_{i1}, \\ldots, a_{im})\\) is the \\(i\\)-th row."
  },
  {
    "objectID": "01_basics.html#some-specific-matrices",
    "href": "01_basics.html#some-specific-matrices",
    "title": "\n1  Basic definitions\n",
    "section": "\n1.2 Some specific matrices",
    "text": "1.2 Some specific matrices\nA matrix is called square matrix if the numbers of rows and columns coincide (i.e., \\(k=m\\)). \\[\\boldsymbol{B} = \\begin{pmatrix}1 & 2 \\\\ 3 & 4 \\end{pmatrix}\\] is a square matrix. A square matrix is called diagonal matrix if all off-diagonal elements are zero. \\[\\boldsymbol{C} = \\begin{pmatrix}1 & 0 \\\\ 0 & 4 \\end{pmatrix}\\] is a diagonal matrix. We also write \\(\\boldsymbol C = \\mathop{\\mathrm{diag}}(1,4)\\). A square matrix is called upper triangular if all elements below the main diagonal are zero, and lower triangular if all elements above the main diagonal are zero. Examples of an upper triangular matrix \\(\\boldsymbol D\\) and a lower triangular matrix \\(\\boldsymbol E\\) are \\[\\boldsymbol{D} = \\begin{pmatrix}1 & 2 \\\\ 0 & 4 \\end{pmatrix},\n\\quad\n\\boldsymbol{E} = \\begin{pmatrix}1 & 0 \\\\ 3 & 4 \\end{pmatrix}.\n\\] The \\(k \\times k\\) diagonal matrix \\[\\boldsymbol{I}_k=\n      \\begin{pmatrix}\n      1      & 0      & \\cdots & 0\\\\\n      0      & 1      & \\cdots & 0\\\\\n      \\vdots & \\vdots & \\ddots & \\vdots\\\\\n      0      & 0      & \\cdots & 1\n      \\end{pmatrix} = \\mathop{\\mathrm{diag}}(1, \\ldots, 1)\\] is called identity matrix of order \\(k\\). The \\(k \\times m\\) matrix \\[\\boldsymbol{0}_{k\\times m}\n       =\\begin{pmatrix}\n             0      & \\cdots & 0\\\\\n            \\vdots &  \\ddots      & \\vdots\\\\\n            0      & \\cdots & 0\n       \\end{pmatrix}\n      \\] is called zero matrix. The zero vector of length \\(k\\) is \\[\\boldsymbol{0}_{k}\n       =\\begin{pmatrix}\n             0   \\\\\n            \\vdots \\\\\n            0\n       \\end{pmatrix}.\n      \\] If the order becomes clear from the context, we omit the indices and write \\(\\boldsymbol I\\) for the identity matrix and \\(\\boldsymbol 0\\) for the zero matrix or zero vector."
  },
  {
    "objectID": "01_basics.html#transposition",
    "href": "01_basics.html#transposition",
    "title": "\n1  Basic definitions\n",
    "section": "\n1.3 Transposition",
    "text": "1.3 Transposition\nThe transpose \\(\\boldsymbol A'\\) of the matrix \\(\\boldsymbol A\\) is obtained by flipping rows and columns on the main diagonal: \\[\\boldsymbol{A}'=\\begin{pmatrix}\n    a_{11} & a_{21} & \\cdots & a_{k1}\\\\\n    a_{12} & a_{22} & \\cdots & a_{k2}\\\\\n    \\vdots & \\vdots &       & \\vdots\\\\\n    a_{1m} & a_{2m} & \\cdots & a_{km}\n\\end{pmatrix}.\\] If \\(\\boldsymbol A\\) is a matrix of order \\(k\\times m\\), then \\(\\boldsymbol A'\\) is a matrix of order \\(m \\times k\\). Example: \\[\\boldsymbol{A}=\\begin{pmatrix}\n        1 & 2\\\\\n        4 & 5\\\\\n        7 & 8\n\\end{pmatrix}\\quad\\Rightarrow\\quad\n\\boldsymbol{A}'=\\begin{pmatrix}\n        1 & 4 & 7\\\\\n        2 & 5 & 8\n\\end{pmatrix}\\] The definition implies that transposing twice produces the original matrix: \\[\n(\\boldsymbol A')' = \\boldsymbol A.\n\\] The transpose of a (column) vector is a row vector: \\[\n\\boldsymbol a' = (a_1, \\ldots, a_k)\n\\]\nA symmetric matrix is a square matrix \\(\\boldsymbol A\\) with \\(\\boldsymbol{A}'=\\boldsymbol{A}\\). An example of a symmetric matrix is \\[\\boldsymbol A = \\begin{aligned}         \\left(\\begin{matrix}1 & 2 \\\\ 2 & 4 \\end{matrix}\\right)\\end{aligned}.\\]"
  },
  {
    "objectID": "01_basics.html#matrices-in-r",
    "href": "01_basics.html#matrices-in-r",
    "title": "\n1  Basic definitions\n",
    "section": "\n1.4 Matrices in R\n",
    "text": "1.4 Matrices in R\n\nLet’s define some matrices in R:\n\nA = matrix(c(1,4,7,2,5,8), nrow = 3, ncol = 2)\nA\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    4    5\n[3,]    7    8\n\nt(A) #transpose of A\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n\nA[3,2] #the (3,2)-entry of A\n\n[1] 8\n\nB = matrix(c(1,2,2,4), nrow = 2, ncol = 2) # another matrix\nall(B == t(B)) #check whether B is symmetric\n\n[1] TRUE\n\ndiag(c(1,4)) #diagonal matrix\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    4\n\ndiag(1, nrow = 3) #identity matrix\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n\nmatrix(0, nrow=2, ncol=5) #matrix of zeros\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    0    0    0    0    0\n[2,]    0    0    0    0    0\n\ndim(A) #number of rows and columns\n\n[1] 3 2"
  },
  {
    "objectID": "02_multiplication.html#matrix-summation",
    "href": "02_multiplication.html#matrix-summation",
    "title": "\n2  Sums and Products\n",
    "section": "\n2.1 Matrix summation",
    "text": "2.1 Matrix summation\nLet \\(\\boldsymbol A\\) and \\(\\boldsymbol B\\) both be matrices of order \\(k \\times m\\). Their sum is defined componentwise: \\[\\boldsymbol{A} + \\boldsymbol{B}\n=\\begin{pmatrix}\na_{11}+ b_{11} & a_{12}+ b_{12} & \\cdots & a_{1m}+ b_{1m} \\\\\na_{21}+ b_{21} & a_{22}+ b_{22} & \\cdots & a_{2m}+ b_{2m} \\\\\n\\vdots        & \\vdots        &       & \\vdots        \\\\\na_{k1}+ b_{k1} & a_{k2}+ b_{k2} & \\cdots & a_{km}+ b_{km}\n\\end{pmatrix}.\\] Only two matrices of the same order can be added. Example: \\[\\boldsymbol{A}=\\begin{pmatrix}2&0\\\\1&5\\\\3&2\\end{pmatrix}\\,,\\quad\n\\boldsymbol{B}=\\begin{pmatrix}-1&1\\\\7&1\\\\-5&2\\end{pmatrix}\\,,\\quad\n\\boldsymbol{A}+\\boldsymbol{B}=\\begin{pmatrix}1&1\\\\8&6\\\\-2&4\\end{pmatrix}\\,.\\]\nThe matrix summation satisfies the following rules: \\[\\begin{array}{@{}rr@{\\ }c@{\\ }l@{}r@{}}\n\\text{(i)}    & \\boldsymbol{A}+\\boldsymbol{B}         &=& \\boldsymbol{B}+\\boldsymbol{A}\\,               &  \\text{(commutativity)} \\\\\n\\text{(ii)}   & (\\boldsymbol{A}+\\boldsymbol{B})+\\boldsymbol{C}  &=& \\boldsymbol{A}+(\\boldsymbol{B}+\\boldsymbol{C})\\,        &  \\text{(associativity)} \\\\\n\\text{(iii)}    & \\boldsymbol A + \\boldsymbol 0 &=& \\boldsymbol A &   {\\text{(identity element)}}     \\\\\n\\text{(iv)}   & (\\boldsymbol A + \\boldsymbol B)' &=& \\boldsymbol A' + \\boldsymbol B'     &\n{\\text{(transposition)}}          \n\\end{array}\\]"
  },
  {
    "objectID": "02_multiplication.html#scalar-matrix-multiplication",
    "href": "02_multiplication.html#scalar-matrix-multiplication",
    "title": "\n2  Sums and Products\n",
    "section": "\n2.2 Scalar-matrix multiplication",
    "text": "2.2 Scalar-matrix multiplication\nThe product of a \\(k \\times m\\) matrix \\(\\boldsymbol{A}\\) with a scalar \\(\\lambda\\in\\mathbb{R}\\) is defined componentwise: \\[\\lambda \\boldsymbol{A} =\n\\begin{pmatrix}\n   \\lambda a_{11}   & \\lambda a_{12} & \\cdots  & \\lambda a_{1n} \\\\\n   \\lambda a_{21}   & \\lambda a_{22} & \\cdots  & \\lambda a_{2n} \\\\\n   \\vdots           & \\vdots &                & \\vdots         \\\\\n   \\lambda a_{m1}   & \\lambda a_{m2} & \\cdots  & \\lambda a_{mn}\n\\end{pmatrix}.\\] Example: \\[\\lambda=2, \\quad \\boldsymbol{A}=\\begin{pmatrix}2&0\\\\1&5\\\\3&2\\end{pmatrix},\\quad\n\\lambda\\boldsymbol{A}=\\begin{pmatrix}4&0\\\\2&10\\\\6&4\\end{pmatrix}.\\] Scalar-matrix multiplication satisfies the distributivity law: \\[\\begin{array}{@{}rr@{\\ }c@{\\ }l@{}r@{}}\n\\text{(i)}    & \\lambda(\\boldsymbol{A}+\\boldsymbol{B})&=& \\lambda\\boldsymbol{A}+\\lambda\\boldsymbol{B}\\, &    \\\\\n\\text{(ii)}   & (\\lambda+\\mu)\\boldsymbol{A} &=& \\lambda\\boldsymbol{A}+\\mu\\boldsymbol{A}\\,     &\n\\end{array}\\]"
  },
  {
    "objectID": "02_multiplication.html#element-by-element-operations-in-r",
    "href": "02_multiplication.html#element-by-element-operations-in-r",
    "title": "\n2  Sums and Products\n",
    "section": "\n2.3 Element-by-element operations in R",
    "text": "2.3 Element-by-element operations in R\nBasic arithmetic operations work on an element-by-element basis in R:\n\nA = matrix(c(2,1,3,0,5,2), ncol=2)\nB = matrix(c(-1,7,-5,1,1,2), ncol=2)\nA+B #matrix summation\n\n     [,1] [,2]\n[1,]    1    1\n[2,]    8    6\n[3,]   -2    4\n\nA-B #matrix subtraction\n\n     [,1] [,2]\n[1,]    3   -1\n[2,]   -6    4\n[3,]    8    0\n\n2*A #scalar-matrix product\n\n     [,1] [,2]\n[1,]    4    0\n[2,]    2   10\n[3,]    6    4\n\nA/2 #division of entries by 2\n\n     [,1] [,2]\n[1,]  1.0  0.0\n[2,]  0.5  2.5\n[3,]  1.5  1.0\n\nA*B #element-wise multiplication\n\n     [,1] [,2]\n[1,]   -2    0\n[2,]    7    5\n[3,]  -15    4"
  },
  {
    "objectID": "02_multiplication.html#vector-vector-multiplication",
    "href": "02_multiplication.html#vector-vector-multiplication",
    "title": "\n2  Sums and Products\n",
    "section": "\n2.4 Vector-vector multiplication",
    "text": "2.4 Vector-vector multiplication\n\n2.4.1 Inner product\nThe inner product (also known as dot product) of two vectors \\(\\boldsymbol{a},\\boldsymbol{b}\\in\\mathbb{R}^k\\) is \\[\\boldsymbol{a}'\\boldsymbol{b} = a_1 b_1+a_2b_2+\\ldots+a_kb_k=\\sum_{i=1}^k a_ib_i\\in\\mathbb{R}.\\] Example: \\[\\boldsymbol{a}=\\begin{pmatrix}1\\\\2\\\\3\\end{pmatrix},\\quad\n\\boldsymbol{b}=\\begin{pmatrix}-2\\\\0\\\\2\\end{pmatrix},\\quad\n\\boldsymbol{a}'\\boldsymbol{b}=1\\cdot(-2)+2\\cdot0+3\\cdot2=4.\\]\nThe inner product is commutative: \\[\\begin{align*}\n  \\boldsymbol a' \\boldsymbol b = \\boldsymbol b' \\boldsymbol a.\n\\end{align*}\\] Two vectors \\(\\boldsymbol a\\) and \\(\\boldsymbol b\\) are called orthogonal if \\(\\boldsymbol a' \\boldsymbol b = 0\\). The vectors \\(\\boldsymbol a\\) and \\(\\boldsymbol b\\) are called orthonormal if, in addition to \\(\\boldsymbol a'\\boldsymbol b\\), we have \\(\\boldsymbol a' \\boldsymbol a = 1\\) and \\(\\boldsymbol b' \\boldsymbol b=1\\).\n\n2.4.2 Outer product\nThe outer product (also known as dyadic product) of two vectors \\(\\boldsymbol{x} \\in \\mathbb R^k\\) and \\(\\boldsymbol{y}\\in\\mathbb{R}^m\\) is \\[\\boldsymbol{x}\\boldsymbol{y}' =\n\\left(\\begin{matrix}\nx_1 y_1 & x_1 y_2  &\\ldots & x_1 y_m \\\\\nx_2 y_1 & x_2 y_2 & \\ldots & x_2 y_m \\\\\n\\vdots   & \\vdots &      & \\vdots    \\\\\nx_k y_1 & x_k y_2 & \\ldots & x_k y_m\n\\end{matrix}\\right)\\in  \\mathbb{R}^{k \\times m}.\\] Example: \\[\\boldsymbol{x}=\\begin{pmatrix}1\\\\2\\end{pmatrix}\\,,\\quad\n\\boldsymbol{y}=\\begin{pmatrix}-2\\\\0\\\\2\\end{pmatrix}\\,,\\quad\n\\boldsymbol{x}\\boldsymbol{y}'=\\left(\\begin{matrix}\n-2 & 0 & 2 \\\\\n-4 & 0 & 4\n\\end{matrix}\\right).\\]\n\n2.4.3 Vector multiplication in R\n\nFor vector multiplication in R, we use the operator %*% (recall that * is already reserved for element-wise multiplication). Let’s implement some multiplications.\n\ny = c(2,7,4,1) #y is treated as a column vector\nt(y) %*% y #the inner product of y with itself\n\n     [,1]\n[1,]   70\n\ny %*% t(y) #the outer product of y with itself\n\n     [,1] [,2] [,3] [,4]\n[1,]    4   14    8    2\n[2,]   14   49   28    7\n[3,]    8   28   16    4\n[4,]    2    7    4    1\n\nc(1,2) %*% t(c(-2,0,2)) #the example from above\n\n     [,1] [,2] [,3]\n[1,]   -2    0    2\n[2,]   -4    0    4"
  },
  {
    "objectID": "02_multiplication.html#matrix-matrix-multiplication",
    "href": "02_multiplication.html#matrix-matrix-multiplication",
    "title": "\n2  Sums and Products\n",
    "section": "\n2.5 Matrix-matrix multiplication",
    "text": "2.5 Matrix-matrix multiplication\nThe matrix product of a \\(k \\times m\\) matrix \\(\\boldsymbol{A}\\) and a \\(m \\times n\\) matrix \\(\\boldsymbol{B}\\) is the \\(k\\times n\\) matrix \\(\\boldsymbol C = \\boldsymbol{A}\\boldsymbol{B}\\) with the components \\[c_{ij} = a_{i1}b_{1j}+a_{i2}b_{2j}+\\ldots+a_{im}b_{mj}=\\sum_{l=1}^m a_{il}b_{lj} = \\boldsymbol a_i' \\boldsymbol b_j,\\] where \\(\\boldsymbol a_i = (a_{i1}, \\ldots, a_{im})'\\) is the \\(i\\)-th row of \\(\\boldsymbol A\\) written as a column vector, and \\(\\boldsymbol b_j = (b_{1j}, \\ldots, b_{mj})'\\) is the \\(j\\)-th column of \\(\\boldsymbol B\\). The full matrix product can be written as \\[\n\\boldsymbol A \\boldsymbol B = \\begin{pmatrix} \\boldsymbol a_1' \\\\ \\vdots \\\\ \\boldsymbol a_k' \\end{pmatrix}\n\\begin{pmatrix} \\boldsymbol b_1 & \\ldots & \\boldsymbol b_n \\end{pmatrix}\n= \\begin{pmatrix} \\boldsymbol a_1' \\boldsymbol b_1 & \\ldots & \\boldsymbol a_1' \\boldsymbol b_n \\\\ \\vdots & & \\vdots \\\\ \\boldsymbol a_k' \\boldsymbol b_1 & \\ldots & \\boldsymbol a_k' \\boldsymbol b_n \\end{pmatrix}.\n\\] The matrix product is only defined if the number of columns of the first matrix equals the number of rows of the second matrix. Therefore, we say that the \\(k \\times m\\) matrix \\(\\boldsymbol A\\) and the \\(m \\times n\\) matrix \\(\\boldsymbol B\\) are conformable for matrix multiplication.\nExample: Let \\[\\begin{aligned}\n    \\boldsymbol{A}=\\begin{pmatrix}\n    1 & 0\\\\\n    0 & 1\\\\\n    2 & 1\n\\end{pmatrix}, \\quad\n\\boldsymbol{B}=\\begin{pmatrix}\n    -1 & 2\\\\\n    -3 & 0\n\\end{pmatrix}.\\end{aligned}\\] Their matrix product is \\[\\begin{aligned}\n\\boldsymbol{A} \\boldsymbol{B} &= \\begin{pmatrix}\n    1 & 0\\\\\n    0 & 1\\\\\n    2 & 1\n\\end{pmatrix} \\begin{pmatrix}\n    -1 & 2\\\\\n    -3 & 0\n\\end{pmatrix} \\\\ &= \\left(\\begin{matrix}1 \\cdot (-1) + 0 \\cdot (-3) & 1 \\cdot 2 + 0 \\cdot 0 \\\\ 0 \\cdot (-1) + 1 \\cdot (-3) & 0 \\cdot 2 + 1 \\cdot 0 \\\\ 2 \\cdot (-1) + 1 \\cdot (-3) & 2 \\cdot 2 + 1 \\cdot 0 \\end{matrix}\\right)\n= \\left(\\begin{matrix}-1 & 2 \\\\ -3 & 0 \\\\ -5 & 4 \\end{matrix}\\right).\\end{aligned}\\]\nThe %*% operator is used in R for matrix-matrix multiplications:\n\nA = matrix(c(1,0,2,0,1,1), ncol=2)\nB = matrix(c(-1,-3,2,0), ncol=2)\nA %*% B\n\n     [,1] [,2]\n[1,]   -1    2\n[2,]   -3    0\n[3,]   -5    4\n\n\nMatrix multiplication is not commutative. In general, we have \\(\\boldsymbol A \\boldsymbol B \\neq \\boldsymbol B \\boldsymbol A\\). Example: \\[\\begin{aligned}\n    \\boldsymbol{A}\\boldsymbol{B} = \\begin{pmatrix} 1 & 2\\\\ 3 & 4\\end{pmatrix}\n    \\begin{pmatrix} 1 & 1\\\\ 1 & 2\\end{pmatrix}\n    &=\n    \\begin{pmatrix} 3 & 5\\\\ 7 & 11\\end{pmatrix}\\,,\\\\\n    \\boldsymbol{B}\\boldsymbol{A} = \\begin{pmatrix} 1 & 1\\\\ 1 & 2\\end{pmatrix}\n    \\begin{pmatrix} 1 & 2\\\\ 3 & 4\\end{pmatrix}\n    &=\n    \\begin{pmatrix} 4 & 6\\\\ 7 & 10\\end{pmatrix}\\,.\\end{aligned}\\] Even if neither of the two matrices contains zeros, the matrix product can give the zero matrix: \\[\\boldsymbol{A}\\boldsymbol{B} = \\begin{pmatrix} 1 & 2\\\\ 2 & 4\\end{pmatrix}\n    \\begin{pmatrix} 2 & -4\\\\ -1 & 2\\end{pmatrix}\n    =\n    \\begin{pmatrix} 0 & 0\\\\ 0 & 0\\end{pmatrix}=\\boldsymbol{0}.\\]\nThe following rules of calculation apply (provided the matrices are conformable): \\[\\begin{array}{rrcl@{}r@{}}\n\\text{(i)}   & \\boldsymbol{A}(\\boldsymbol{B}\\boldsymbol{C})       & = & (\\boldsymbol{A}\\boldsymbol{B})\\boldsymbol{C}\\,     &\\text{(associativity)} \\\\\n\\text{(ii)}  &   \\boldsymbol{A}(\\boldsymbol{B}+\\boldsymbol{D})    & = & \\boldsymbol{A}\\boldsymbol{B}+\\boldsymbol{A}\\boldsymbol{D}\\,  & \\text{(distributivity)} \\\\\n\\text{(iii)}   &   (\\boldsymbol{B}+\\boldsymbol{D})\\boldsymbol{C}    & = & \\boldsymbol{B}\\boldsymbol{C}+\\boldsymbol{D}\\boldsymbol{C}\\,  & \\text{(distributivity)} \\\\\n\\text{(iv)}  &   \\boldsymbol{A}(\\lambda \\boldsymbol{B}) & = & \\lambda(\\boldsymbol{A}\\boldsymbol{B})\\,  & \\text{(scalar commutativity)}\\\\\n\\text{(v)}  & \\boldsymbol{A}\\boldsymbol{I}_{n} & = & \\boldsymbol{A}\\,,              & \\text{(identity element)}\\\\\n\\text{(vi)} & \\boldsymbol{I}_{m}\\boldsymbol{A} & = & \\boldsymbol{A}\\,               &\n\\text{(identity element)}     \\\\\n\\text{(vii)} &   (\\boldsymbol{A}\\boldsymbol{B})'  & = & \\boldsymbol{B}'\\boldsymbol{A}'\\,         & \\text{(product transposition)} \\\\\n\\text{(viii)} &   (\\boldsymbol{A}\\boldsymbol{B} \\boldsymbol C)'  & = & \\boldsymbol C' \\boldsymbol{B}'\\boldsymbol{A}'\\,  & \\text{(product transposition)}\n\\end{array}\\]"
  },
  {
    "objectID": "03_inverse.html#linear-combination",
    "href": "03_inverse.html#linear-combination",
    "title": "\n3  Rank and inverse\n",
    "section": "\n3.1 Linear combination",
    "text": "3.1 Linear combination\nLet \\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\) be vectors of the same order, and let \\(\\lambda_1, \\ldots, \\lambda_n\\) be scalars. The vector \\[ \\lambda_1 \\boldsymbol x_1 + \\lambda_2 \\boldsymbol x_2 + \\ldots + \\lambda_n \\boldsymbol x_n \\]\nis called linear combination of \\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\). A linear combination can also be written as a matrix-vector product. Let \\(\\boldsymbol X =\\begin{pmatrix} \\boldsymbol x_1 & \\ldots & \\boldsymbol x_n \\end{pmatrix}\\) be the matrix with columns \\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\), and let \\(\\boldsymbol \\lambda = (\\lambda_1, \\ldots, \\lambda_n)'\\). Then, \\[ \\lambda_1 \\boldsymbol x_1 + \\lambda_2 \\boldsymbol x_2 + \\ldots + \\lambda_n \\boldsymbol x_n = \\boldsymbol X \\boldsymbol \\lambda.\\] The vectors \\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\) are called linearly dependent if at least one can be written as a linear combination of the others. That is, there exists a nonzero vector \\(\\boldsymbol \\lambda\\) with \\[\n\\boldsymbol X \\boldsymbol \\lambda = \\lambda_1 \\boldsymbol x_1 + \\ldots + \\lambda_n \\boldsymbol x_n = \\boldsymbol 0.\n\\] The vectors \\(\\boldsymbol x_1, \\ldots, \\boldsymbol x_n\\) are called linearly independent if \\[\n\\boldsymbol X \\boldsymbol \\lambda = \\lambda_1 \\boldsymbol x_1 + \\ldots + \\lambda_n \\boldsymbol x_n \\neq \\boldsymbol 0\n\\] for all nonzero vectors \\(\\boldsymbol \\lambda\\).\nTo check whether the vectors are linearly independent, we can solve the system of equations \\(\\boldsymbol X \\boldsymbol \\lambda = \\boldsymbol 0\\) by Gaussian elimination. If \\(\\boldsymbol \\lambda = \\boldsymbol 0\\) is the only solution, then the columns of \\(\\boldsymbol X\\) are linearly independent. If there is a solution \\(\\boldsymbol \\lambda\\) with \\(\\boldsymbol \\lambda \\neq \\boldsymbol 0\\), then the columns of \\(\\boldsymbol X\\) are linearly dependent."
  },
  {
    "objectID": "03_inverse.html#column-rank",
    "href": "03_inverse.html#column-rank",
    "title": "\n3  Rank and inverse\n",
    "section": "\n3.2 Column rank",
    "text": "3.2 Column rank\nThe rank of a \\(k \\times m\\) matrix \\(\\boldsymbol A = \\begin{pmatrix} \\boldsymbol a_1 & \\ldots & \\boldsymbol a_m \\end{pmatrix}\\), written as \\(\\mathop{\\mathrm{rank}}(\\boldsymbol A)\\), is the number of linearly independent columns \\(\\boldsymbol a_i\\). We say that \\(\\boldsymbol A\\) has full column rank if \\(\\mathop{\\mathrm{rank}}(\\boldsymbol X) = m\\).\nThe identity matrix \\(\\boldsymbol I_k\\) has full column rank (i.e., \\(\\mathop{\\mathrm{rank}}(\\boldsymbol I_n) = k\\)). As another example, consider \\[\n\\boldsymbol X = \\begin{pmatrix} 2 & 1 & 4 \\\\ 0 & 1 & 2 \\end{pmatrix},\n\\] which has linearly dependent columns since the third column is a linear combination of the first two columns: \\[\n\\begin{pmatrix} 4 \\\\ 2 \\end{pmatrix} = 1 \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} + 2 \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}.\n\\] The first two columns are linearly independent since \\(\\lambda_1 = 0\\) and \\(\\lambda_2 = 0\\) are the only solutions to the equation \\[\n\\lambda_1 \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} + \\lambda_2 \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}.\n\\] Therefore, we have \\(\\mathop{\\mathrm{rank}}(\\boldsymbol X) = 2\\), i.e., \\(\\boldsymbol X\\) does not have a full column rank.\nSome useful properties are\n\n\\(\\mathop{\\mathrm{rank}}(\\boldsymbol A) \\leq \\min(k,m)\\)\n\\(\\mathop{\\mathrm{rank}}(\\boldsymbol A) = \\mathop{\\mathrm{rank}}(\\boldsymbol A')\\)\n\\(\\mathop{\\mathrm{rank}}(\\boldsymbol A \\boldsymbol B) = \\min(\\mathop{\\mathrm{rank}}(\\boldsymbol A), \\mathop{\\mathrm{rank}}(\\boldsymbol B))\\)\n\n\\(\\mathop{\\mathrm{rank}}(\\boldsymbol A) = \\mathop{\\mathrm{rank}}(\\boldsymbol A' \\boldsymbol A) = \\mathop{\\mathrm{rank}}(\\boldsymbol A \\boldsymbol A')\\).\n\nWe can use the qr() function to extract the rank in R. Let’s compute the rank of the matrices \\[\n\\boldsymbol A = \\begin{pmatrix}\n1 & 2 & 3 \\\\ 3 & 9 & 1 \\\\ 0 & 11 & 5\n\\end{pmatrix},\n\\] \\(\\boldsymbol B = \\boldsymbol I_3\\), and \\(\\boldsymbol X\\) from the example above:\n\nA = matrix(c(1,3,0,2,9,11,3,1,5), nrow=3)\nqr(A)$rank\n\n[1] 3\n\nB = matrix(c(1,1,1,1,1,1,1,1,1), nrow=3)\nqr(B)$rank\n\n[1] 1\n\nX = matrix(c(2,0,1,1,4,2), ncol=3)\nqr(X)$rank\n\n[1] 2"
  },
  {
    "objectID": "03_inverse.html#nonsingular-matrix",
    "href": "03_inverse.html#nonsingular-matrix",
    "title": "\n3  Rank and inverse\n",
    "section": "\n3.3 Nonsingular matrix",
    "text": "3.3 Nonsingular matrix\nA square \\(k \\times k\\) matrix \\(\\boldsymbol A\\) is called nonsingular if it has full rank, i.e., \\(\\mathop{\\mathrm{rank}}(\\boldsymbol A) = k\\). Conversely, \\(\\boldsymbol A\\) is called singular if it does not have full rank, i.e., \\(\\mathop{\\mathrm{rank}}(\\boldsymbol A) &lt; k\\)."
  },
  {
    "objectID": "03_inverse.html#determinant",
    "href": "03_inverse.html#determinant",
    "title": "\n3  Rank and inverse\n",
    "section": "\n3.4 Determinant",
    "text": "3.4 Determinant\nConsider a square \\(k \\times k\\) matrix \\(\\boldsymbol A\\). The determinant \\(\\det(\\boldsymbol A)\\) is a measure of the volume of the geometric object formed by the columns of \\(\\boldsymbol A\\) (a parallelogram for \\(k=2\\), a parallelepiped for \\(k=3\\), a hyper-parallelepiped for \\(k&gt;3\\)). For \\(2 \\times 2\\) matrices, the determinant is easy to calculate: \\[\n\\boldsymbol A = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}, \\quad\n\\det(\\boldsymbol A) = ad - bc.\n\\]\nIf \\(\\boldsymbol A\\) is triangular (upper or lower), the determinant is the product of the diagonal entries, i.e., \\(\\det(\\boldsymbol A) = \\prod_{i=1}^k a_{ii}\\). Hence, Gaussian elimination can be used to compute the determinant by transforming the matrix to a triangular one. The exact definition of the determinant is technical and of little importance to us. A useful relation is the following: \\[\\begin{align*}\n  \\det(\\boldsymbol A) = 0 \\quad &\\Leftrightarrow \\quad \\boldsymbol A \\ \\text{is singular} \\\\\n   \\det(\\boldsymbol A) \\neq 0 \\quad &\\Leftrightarrow \\quad \\boldsymbol A \\ \\text{is nonsingular}.\n\\end{align*}\\]\nIn R, we have the det() function to compute the determinant:\n\ndet(A)\n\n[1] 103\n\ndet(B)\n\n[1] 0\n\n\nSince \\(\\det(\\boldsymbol A) \\neq 0\\) and \\(\\det(\\boldsymbol B) = 0\\), we conclude that \\(\\boldsymbol A\\) is nonsingular and \\(\\boldsymbol B\\) is singular."
  },
  {
    "objectID": "03_inverse.html#inverse-matrix",
    "href": "03_inverse.html#inverse-matrix",
    "title": "\n3  Rank and inverse\n",
    "section": "\n3.5 Inverse matrix",
    "text": "3.5 Inverse matrix\nThe inverse \\(\\boldsymbol{A}^{-1}\\) of a square \\(k \\times k\\) matrix \\(\\boldsymbol A\\) is defined by the property \\[\\boldsymbol{A} \\boldsymbol{A}^{-1} = \\boldsymbol{A}^{-1} \\boldsymbol{A} =\\boldsymbol{I}_k.\\] When multiplied from the left or the right, the inverse matrix produces the identity matrix. The inverse exists if and only if \\(\\boldsymbol{A}\\) is nonsingular, i.e., \\(\\det(\\boldsymbol A) \\neq 0\\). Therefore, a nonsingular matrix is also called invertible matrix. Note that only square matrices can be inverted.\nFor \\(2 \\times 2\\) matrices, there exists a simple formula: \\[\\boldsymbol{A}^{-1} = \\frac{1}{\\det(\\boldsymbol{A})} \\begin{pmatrix}d&-b\\\\-c&a\\end{pmatrix}\\,,\\] where \\(\\det(\\boldsymbol{A}) = ad - bc\\). We swap the main diagonal elements, reverse the sign of the off-diagonal elements, and divide all entries by the determinant. Example: \\[\\displaystyle\\boldsymbol{A}=\\begin{pmatrix}5&6\\\\1&2\\end{pmatrix}\\] We have \\(\\det(\\boldsymbol{A}) = ad-bc=5\\cdot2-6\\cdot1=4\\), and \\[\\boldsymbol{A}^{-1}= \\frac{1}{4} \\cdot \\begin{pmatrix}2&-6\\\\-1&5\\end{pmatrix}.\\] Indeed, \\(\\boldsymbol A^{-1}\\) is the inverse of \\(\\boldsymbol A\\) since \\[\\boldsymbol{A}\\boldsymbol{A}^{-1}\n=\\begin{pmatrix}5&6\\\\1&2\\end{pmatrix} \\cdot \\frac{1}{4} \\cdot \\begin{pmatrix}2&-6\\\\-1&5\\end{pmatrix}\n=\\frac{1}{4} \\cdot \\begin{pmatrix}4&0\\\\0&4\\end{pmatrix}\n= \\left(\\begin{matrix}1 & 0 \\\\ 0 & 1 \\end{matrix}\\right)\n= \\boldsymbol{I}_2.\\]\nOne way to calculate the inverse of higher order square matrices is to solve equation \\(\\boldsymbol A \\boldsymbol A^{-1} = \\boldsymbol I\\) with Gaussian elimination. R can compute the inverse matrix quickly using the function solve():\n\nsolve(A) #inverse if A\n\n           [,1]        [,2]        [,3]\n[1,]  0.3300971  0.22330097 -0.24271845\n[2,] -0.1456311  0.04854369  0.07766990\n[3,]  0.3203883 -0.10679612  0.02912621\n\n\nWe have the following relationship between invertibility, rank, and determinant of a square matrix \\(\\boldsymbol A\\):\n\\[\\begin{align*}\n  &\\boldsymbol A \\ \\text{is nonsingular} \\\\\n  \\Leftrightarrow \\quad &\\text{all columns of} \\ \\boldsymbol A \\ \\text{are linearly independent} \\\\\n  \\Leftrightarrow \\quad &\\boldsymbol A \\ \\text{has full column rank} \\\\\n  \\Leftrightarrow \\quad &\\text{the determinant is nonzero} \\ (\\det(\\boldsymbol A) \\neq 0).\n\\end{align*}\\]\nSimilarly, \\[\\begin{align*}\n  &\\boldsymbol A \\ \\text{is singular} \\\\\n  \\Leftrightarrow \\quad &\\boldsymbol A \\ \\text{has linearly dependent columns} \\\\\n  \\Leftrightarrow \\quad &\\boldsymbol A \\ \\text{does not have full rank} \\\\\n  \\Leftrightarrow \\quad &\\text{the determinant is zero} \\ (\\det(\\boldsymbol A) = 0).\n\\end{align*}\\]\nBelow you will find some important properties for nonsingular matrices:\n\n\\((\\boldsymbol{A}^{-1})^{-1} = \\boldsymbol{A}\\)\n\\((\\boldsymbol{A}')^{-1} = (\\boldsymbol{A}^{-1})'\\)\n\n\\((\\lambda\\boldsymbol{A})^{-1} = \\frac{1}{\\lambda}\\boldsymbol{A}^{-1}\\) for any \\(\\lambda \\neq 0\\)\n\n\\(\\det(\\boldsymbol A^{-1}) = \\frac{1}{\\det(\\boldsymbol A)}\\)\n\\((\\boldsymbol{A} \\boldsymbol{B})^{-1} = \\boldsymbol{B}^{-1} \\boldsymbol{A}^{-1}\\)\n\\((\\boldsymbol A \\boldsymbol B \\boldsymbol C)^{-1} = \\boldsymbol C^{-1} \\boldsymbol B^{-1} \\boldsymbol A^{-1}\\)\nIf \\(\\boldsymbol{A}\\) is symmetric, then \\(\\boldsymbol{A}^{-1}\\) is symmetric."
  },
  {
    "objectID": "04_furtherconcepts.html#trace",
    "href": "04_furtherconcepts.html#trace",
    "title": "\n4  Advanced concepts\n",
    "section": "\n4.1 Trace",
    "text": "4.1 Trace\nThe trace of a \\(k \\times k\\) square matrix \\(\\boldsymbol A\\) is the sum of the diagonal entries: \\[\\mathop{\\mathrm{tr}}(\\boldsymbol A) = \\sum_{i=1}^n a_{ii}\\] Example: \\[\n\\boldsymbol A = \\begin{pmatrix}\n1 & 2 & 3 \\\\ 3 & 9 & 1 \\\\ 0 & 11 & 5\n\\end{pmatrix} \\quad \\Rightarrow \\quad \\mathop{\\mathrm{tr}}(\\boldsymbol A) = 1+9+5 = 15\n\\] In Rwe have\n\nA = matrix(c(1,3,0,2,9,11,3,1,5), nrow=3)\nsum(diag(A))  #trace = sum of diagonal entries\n\n[1] 15\n\n\nThe following properties hold for square matrices \\(\\boldsymbol A\\) and \\(\\boldsymbol B\\) and scalars \\(\\lambda\\):\n\n\\(\\mathop{\\mathrm{tr}}(\\lambda \\boldsymbol A) = \\lambda \\mathop{\\mathrm{tr}}(\\boldsymbol A)\\)\n\\(\\mathop{\\mathrm{tr}}(\\boldsymbol A + \\boldsymbol B) = \\mathop{\\mathrm{tr}}(\\boldsymbol A) + \\mathop{\\mathrm{tr}}(\\boldsymbol B)\\)\n\\(\\mathop{\\mathrm{tr}}(\\boldsymbol A') = \\mathop{\\mathrm{tr}}(\\boldsymbol A)\\)\n\\(\\mathop{\\mathrm{tr}}(\\boldsymbol I_k) = k\\)\n\nFor \\(\\boldsymbol A\\in \\mathbb R^{k \\times m}\\) and \\(\\boldsymbol B \\in \\mathbb R^{m \\times k}\\) we have \\[\n\\mathop{\\mathrm{tr}}(\\boldsymbol A \\boldsymbol B) = \\mathop{\\mathrm{tr}}(\\boldsymbol B \\boldsymbol A).\n\\]"
  },
  {
    "objectID": "04_furtherconcepts.html#idempotent-matrx",
    "href": "04_furtherconcepts.html#idempotent-matrx",
    "title": "\n4  Advanced concepts\n",
    "section": "\n4.2 Idempotent matrx",
    "text": "4.2 Idempotent matrx\nThe square matrix \\(\\boldsymbol A\\) is called idempotent if \\(\\boldsymbol A \\boldsymbol A = \\boldsymbol A\\). The identity matrix is idempotent: \\(\\boldsymbol I_n \\boldsymbol I_n = \\boldsymbol I_n\\). Another example is the matrix \\[\n\\boldsymbol A = \\begin{pmatrix}\n4 & -1 \\\\ 12 & -3\n\\end{pmatrix}.\n\\] We have \\[\\begin{align*}\n\\boldsymbol A \\boldsymbol A\n&= \\begin{pmatrix}\n4 & -1 \\\\ 12 & -3\n\\end{pmatrix}\n\\begin{pmatrix}\n4 & -1 \\\\ 12 & -3\n\\end{pmatrix} \\\\\n&= \\begin{pmatrix}\n16-12 & -4+3 \\\\ 48-36 & -12+9\n\\end{pmatrix} \\\\\n&= \\begin{pmatrix}\n4 & -1 \\\\ 12 & -3\n\\end{pmatrix}\n= \\boldsymbol A.\n\\end{align*}\\]"
  },
  {
    "objectID": "04_furtherconcepts.html#eigendecomposition",
    "href": "04_furtherconcepts.html#eigendecomposition",
    "title": "\n4  Advanced concepts\n",
    "section": "\n4.3 Eigendecomposition",
    "text": "4.3 Eigendecomposition\n\n4.3.1 Eigenvalues\nAn eigenvalue \\(\\lambda\\) of a \\(k \\times k\\) square matrix is a solution to the equation \\[\n  \\det(\\lambda \\boldsymbol I_k - \\boldsymbol A) = 0.\n\\] The function \\(f(\\lambda) = \\det(\\lambda \\boldsymbol I_k - \\boldsymbol A)\\) has exactly \\(k\\) roots so that \\(\\det(\\lambda \\boldsymbol I_k - \\boldsymbol A) = 0\\) has exactly \\(k\\) solutions. The solutions \\(\\lambda_1, \\ldots, \\lambda_k\\) are the \\(k\\) eigenvalues of \\(\\boldsymbol A\\).\nMost applications of eigenvalues in econometrics concern symmetric matrices. In this case, all eigenvalues are real-valued. In the case of non-symmetric matrices, some eigenvalues may be complex-valued.\nUseful properties of the eigenvalues of a symmetric \\(k \\times k\\) matrix are:\n\n\\(\\det(\\boldsymbol A) = \\lambda_1 \\cdot \\ldots \\cdot \\lambda_k\\)\n\\(\\mathop{\\mathrm{tr}}(\\boldsymbol A) = \\lambda_1 + \\ldots + \\lambda_k\\)\n\n\\(\\boldsymbol A\\) is nonsingular if and only if all eigenvalues are nonzero\n\n\\(\\boldsymbol A \\boldsymbol B\\) and \\(\\boldsymbol B \\boldsymbol A\\) have the same eigenvalues.\n\n4.3.2 Eigenvectors\nIf \\(\\lambda_i\\) is an eigenvalue of \\(\\boldsymbol A\\), then \\(\\lambda_i \\boldsymbol I_k - \\boldsymbol A\\) is singular, which implies that there exists a linear combination vector \\(\\boldsymbol v_i\\) with \\((\\lambda_i \\boldsymbol I_k - \\boldsymbol A) \\boldsymbol v_i = \\boldsymbol 0\\). Equivalently, \\[\n  \\boldsymbol A \\boldsymbol v_i = \\lambda_i \\boldsymbol v_i,\n\\]\nwhich can be solved by Gaussian elimination. It is convenient to normalize any solution such that \\(\\boldsymbol v_i'\\boldsymbol v_i = 1\\). The solutions \\(\\boldsymbol v_1, \\ldots, \\boldsymbol v_k\\) are called eigenvectors of \\(\\boldsymbol A\\) to corresponding eigenvalues \\(\\lambda_1, \\ldots, \\lambda_k\\).\n\n4.3.3 Spectral decomposition\nIf \\(\\boldsymbol A\\) is symmetric, then \\(\\boldsymbol v_1, \\ldots, \\boldsymbol v_k\\) are pairwise orthogonal (i.e., \\(\\boldsymbol v_i' \\boldsymbol v_j = 0\\) for \\(i \\neq j\\)). Let \\(\\boldsymbol V = \\begin{pmatrix} \\boldsymbol v_1 & \\ldots & \\boldsymbol v_k \\end{pmatrix}\\) be the \\(k \\times k\\) matrix of eigenvectors and let \\(\\boldsymbol \\Lambda = \\mathop{\\mathrm{diag}}(\\lambda_1, \\ldots, \\lambda_k)\\) be the \\(k \\times k\\) diagonal matrix with the eigenvalues on the main diagonal. Then, we can write \\[\n  \\boldsymbol A = \\boldsymbol V \\boldsymbol \\Lambda \\boldsymbol V',\n\\]\nwhich is called the spectral decomposition of \\(\\boldsymbol A\\). The matrix of eigenvalues can be written as \\(\\boldsymbol \\Lambda = \\boldsymbol V' \\boldsymbol A \\boldsymbol V\\).\n\n4.3.4 Eigendecomposition in R\n\nThe function eigen() computes the eigenvalues and corresponding eigenvectors.\n\nB=t(A)%*%A \nB #A'A is symmetric\n\n     [,1] [,2] [,3]\n[1,]   10   29    6\n[2,]   29  206   70\n[3,]    6   70   35\n\neigen(B) #eigenvalues and eigenvector matrix\n\neigen() decomposition\n$values\n[1] 234.827160  12.582227   3.590613\n\n$vectors\n           [,1]       [,2]       [,3]\n[1,] -0.1293953 -0.5312592  0.8372697\n[2,] -0.9346164 -0.2167553 -0.2819739\n[3,] -0.3312839  0.8190121  0.4684764"
  },
  {
    "objectID": "04_furtherconcepts.html#definite-matrix",
    "href": "04_furtherconcepts.html#definite-matrix",
    "title": "\n4  Advanced concepts\n",
    "section": "\n4.4 Definite matrix",
    "text": "4.4 Definite matrix\nThe \\(k \\times k\\) square matrix \\(\\boldsymbol{A}\\) is called positive definite if \\[\\boldsymbol{c}'\\boldsymbol{Ac}&gt;0\\] holds for all nonzero vectors \\(\\boldsymbol{c}\\in \\mathbb{R}^k\\). If \\[\\boldsymbol{c}'\\boldsymbol{Ac}\\geq 0\\]\nfor all vectors \\(\\boldsymbol{c}\\in \\mathbb{R}^k\\), the matrix is called positive semi-definite. Analogously, \\(\\boldsymbol A\\) is called negative definite if \\(\\boldsymbol{c}'\\boldsymbol{Ac}&lt;0\\) and negative semi-definite if \\(\\boldsymbol{c}'\\boldsymbol{Ac}\\leq 0\\) for all nonzero vectors \\(\\boldsymbol c \\in \\mathbb R^k\\). A matrix that is neither positive semi-definite nor negative semi-definite is called indefinite\nThe definiteness property of a symmetric matrix \\(\\boldsymbol A\\) can be determined using its eigenvalues:\n\n\n\\(\\boldsymbol A\\) is positive definite  \\(\\Leftrightarrow\\)  all eigenvalues of \\(\\boldsymbol A\\) are strictly positive\n\n\\(\\boldsymbol A\\) is negative definite  \\(\\Leftrightarrow\\)   all eigenvalues of \\(\\boldsymbol A\\) are strictly negative\n\n\\(\\boldsymbol A\\) is positive semi-definite  \\(\\Leftrightarrow\\)   all eigenvalues of \\(\\boldsymbol A\\) are non-negative\n\n\\(\\boldsymbol A\\) is negative semi-definite  \\(\\Leftrightarrow\\)   all eigenvalues of \\(\\boldsymbol A\\) are non-positive\n\n\neigen(B)$values #B is positive definite (all eigenvalues positive)\n\n[1] 234.827160  12.582227   3.590613\n\n\nThe matrix analog of a positive or negative number (scalar) is a positive definite or negative definite matrix. Therefore, we use the notation\n\n\n\\(\\boldsymbol A &gt; 0\\)  if \\(\\boldsymbol A\\) is positive definite\n\n\\(\\boldsymbol A &lt; 0\\)  if \\(\\boldsymbol A\\) is negative definite\n\n\\(\\boldsymbol A \\geq 0\\)  if \\(\\boldsymbol A\\) is positive semi-definite\n\n\\(\\boldsymbol A \\leq 0\\)  if \\(\\boldsymbol A\\) is negative semi-definite\n\nThe notation \\(\\boldsymbol A &gt; \\boldsymbol B\\) means that the matrix \\(\\boldsymbol A - \\boldsymbol B\\) is positive definite."
  },
  {
    "objectID": "04_furtherconcepts.html#cholesky-decomposition",
    "href": "04_furtherconcepts.html#cholesky-decomposition",
    "title": "\n4  Advanced concepts\n",
    "section": "\n4.5 Cholesky decomposition",
    "text": "4.5 Cholesky decomposition\nAny positive definite and symmetric matrix \\(\\boldsymbol B\\) can be written as \\[\n  \\boldsymbol B = \\boldsymbol P \\boldsymbol P',\n\\] where \\(P\\) is a lower triangular matrix with strictly positive diagonal entries \\(p_{jj} &gt; 0\\). This representation is called Cholesky decomposition. The matrix \\(\\boldsymbol P\\) is unique. For a \\(2 \\times 2\\) matrix \\(\\boldsymbol B\\) we have \\[\\begin{align*}\n\\begin{pmatrix} b_{11} & b_{12} \\\\ b_{21} & b_{22} \\end{pmatrix}\n&= \\begin{pmatrix} p_{11} & 0 \\\\ p_{21} & p_{22} \\end{pmatrix}\n\\begin{pmatrix} p_{11} & p_{21} \\\\ 0 & p_{22} \\end{pmatrix} \\\\\n&= \\begin{pmatrix} p_{11}^2 & p_{11} p_{21} \\\\ p_{11} p_{21} & p_{21}^2 + p_{22}^2 \\end{pmatrix},\n\\end{align*}\\] which implies \\(p_{11} = \\sqrt{b_{11}}\\), \\(p_{21} = b_{21}/p_{11}\\), and \\(p_{22} = \\sqrt{b_{22} - p_{21}^2}\\). For a \\(3 \\times 3\\) matrix we obtain\n\\[\\begin{align*}\n\\begin{pmatrix} b_{11} & b_{12} & b_{31} \\\\ b_{21} & b_{22} & b_{23} \\\\ b_{31} & b_{32} & b_{33} \\end{pmatrix}\n= \\begin{pmatrix} p_{11} & 0 & 0 \\\\ p_{21} & p_{22} & 0 \\\\ p_{31} & p_{32} & p_{33} \\end{pmatrix}\n\\begin{pmatrix} p_{11} & p_{21} & p_{31} \\\\ 0 & p_{22} & p_{32} \\\\ 0 & 0 & p_{33}\\end{pmatrix} \\\\\n= \\begin{pmatrix} p_{11}^2 & p_{11} p_{21} & p_{11} p_{31} \\\\ p_{11} p_{21} & p_{21}^2 + p_{22}^2 & p_{21} p_{31} + p_{22} p_{32} \\\\ p_{11}p_{31} & p_{21}p_{31} + p_{22}p_{32} & p_{31}^2 + p_{32}^2  + p_{33}^2\\end{pmatrix},\n\\end{align*}\\] which implies\n\\[\\begin{gather*}\np_{11}=\\sqrt{b_{11}}, \\ \\ p_{21} = \\frac{b_{21}}{p_{11}}, \\ \\ p_{31} = \\frac{b_{31}}{p_{11}}, \\ \\ p_{22} = \\sqrt{b_{22}-p_{21}^2}, \\\\\np_{32}= \\frac{b_{32}-p_{21}p_{31}}{p_{22}}, \\ \\ p_{33} = \\sqrt{b_{33} - p_{31}^2 - p_{32}^2}.\n\\end{gather*}\\]\nLet’s compute the Cholesky decomposition of \\[\n\\boldsymbol B = \\begin{pmatrix} 1 & -0.5 & 0.6 \\\\ -0.5 & 1 & 0.25 \\\\ 0.6 & 0.25 & 1 \\end{pmatrix}\n\\] using the R function chol():\n\nB = matrix(c(1, -0.5, 0.6, -0.5, 1, 0.25, 0.6, 0.25, 1), ncol=3)\nchol(B)\n\n     [,1]       [,2]      [,3]\n[1,]    1 -0.5000000 0.6000000\n[2,]    0  0.8660254 0.6350853\n[3,]    0  0.0000000 0.4864840"
  },
  {
    "objectID": "04_furtherconcepts.html#vectorization",
    "href": "04_furtherconcepts.html#vectorization",
    "title": "\n4  Advanced concepts\n",
    "section": "\n4.6 Vectorization",
    "text": "4.6 Vectorization\nThe vectorization operator \\(\\mathop{\\mathrm{vec}}()\\) stacks the matrix entries column-wise into a large vector. The vectorized \\(k \\times m\\) matrix \\(\\boldsymbol A\\) is the \\(km \\times 1\\) vector \\[\n\\mathop{\\mathrm{vec}}(\\boldsymbol A) = (a_{11}, \\ldots, a_{k1}, a_{12}, \\ldots, a_{k2}, \\ldots, a_{1m}, \\ldots, a_{km})'.\n\\]\n\nc(A) #vectorize the matrix A\n\n[1]  1  3  0  2  9 11  3  1  5"
  },
  {
    "objectID": "04_furtherconcepts.html#kronecker-product",
    "href": "04_furtherconcepts.html#kronecker-product",
    "title": "\n4  Advanced concepts\n",
    "section": "\n4.7 Kronecker product",
    "text": "4.7 Kronecker product\nThe Kronecker product \\(\\otimes\\) multiplies each element of the left-hand side matrix with the entire matrix on the right-hand side. For a \\(k \\times m\\) matrix \\(\\boldsymbol A\\) and a \\(r \\times s\\) matrix \\(\\boldsymbol B\\), we get the \\(kr\\times ms\\) matrix \\[\nA \\otimes B = \\begin{pmatrix} a_{11}\\boldsymbol B & \\ldots & a_{1m}\\boldsymbol B \\\\ \\vdots & & \\vdots \\\\ a_{k1}\\boldsymbol B & \\ldots & a_{km}\\boldsymbol B \\end{pmatrix},\n\\] where each entry \\(a_{ij} \\boldsymbol B\\) is a \\(r \\times s\\) matrix.\n\nA %x% B #Kronecker product in R\n\n      [,1]  [,2] [,3] [,4]  [,5]  [,6] [,7]  [,8] [,9]\n [1,]  1.0 -0.50 0.60  2.0 -1.00  1.20  3.0 -1.50 1.80\n [2,] -0.5  1.00 0.25 -1.0  2.00  0.50 -1.5  3.00 0.75\n [3,]  0.6  0.25 1.00  1.2  0.50  2.00  1.8  0.75 3.00\n [4,]  3.0 -1.50 1.80  9.0 -4.50  5.40  1.0 -0.50 0.60\n [5,] -1.5  3.00 0.75 -4.5  9.00  2.25 -0.5  1.00 0.25\n [6,]  1.8  0.75 3.00  5.4  2.25  9.00  0.6  0.25 1.00\n [7,]  0.0  0.00 0.00 11.0 -5.50  6.60  5.0 -2.50 3.00\n [8,]  0.0  0.00 0.00 -5.5 11.00  2.75 -2.5  5.00 1.25\n [9,]  0.0  0.00 0.00  6.6  2.75 11.00  3.0  1.25 5.00"
  },
  {
    "objectID": "04_furtherconcepts.html#vector-and-matrix-norm",
    "href": "04_furtherconcepts.html#vector-and-matrix-norm",
    "title": "\n4  Advanced concepts\n",
    "section": "\n4.8 Vector and matrix norm",
    "text": "4.8 Vector and matrix norm\nA norm \\(\\|\\cdot\\|\\) of a vector or a matrix is a measure of distance from the origin. The most commonly used norms are the Euclidean vector norm \\[\n  \\|\\boldsymbol a\\| = \\sqrt{\\boldsymbol a' \\boldsymbol a} = \\sqrt{\\sum_{i=1}^k a_i^2}\n\\] for \\(\\boldsymbol a \\in \\mathbb R^k\\), and the Frobenius matrix norm \\[\n  \\|\\boldsymbol A \\| = \\sqrt{\\sum_{i=1}^k \\sum_{j=1}^m a_{ij}^2}\n\\] for \\(\\boldsymbol A \\in \\mathbb R^{k \\times m}\\).\nA norm satisfies the following properties:\n\n\n\\(\\|\\lambda \\boldsymbol A\\| = |\\lambda| \\|\\boldsymbol A\\|\\) for any scalar \\(\\lambda\\) (absolute homogeneity)\n\n\\(\\|\\boldsymbol A + \\boldsymbol B\\| \\leq \\|\\boldsymbol A\\| + \\|\\boldsymbol B\\|\\) (triangle inequality)\n\n\\(\\|\\boldsymbol A\\| = 0\\) implies \\(\\boldsymbol A = \\boldsymbol 0\\) (definiteness)"
  },
  {
    "objectID": "05_calculus.html#gradient",
    "href": "05_calculus.html#gradient",
    "title": "5  Matrix calculus",
    "section": "5.1 Gradient",
    "text": "5.1 Gradient\nThe first derivatives vector or gradient is \\[\n\\frac{\\partial f(\\boldsymbol \\beta)}{\\partial\\boldsymbol{\\beta}}  = \\begin{pmatrix}\\frac{\\partial f(\\boldsymbol \\beta)}{\\partial \\beta_1} \\\\ \\vdots \\\\ \\frac{\\partial f(\\boldsymbol \\beta)}{\\partial \\beta_k} \\end{pmatrix}\n\\] If the gradient is evaluated at some particular value \\(\\boldsymbol \\beta = \\boldsymbol b\\), we write \\[\n\\frac{\\partial f}{\\partial\\boldsymbol{\\beta}}(\\boldsymbol b)\n\\] Useful properties for inner product and sandwich forms are \\[\\begin{align*}\n(i)& \\quad &&\\frac{\\partial (\\boldsymbol a' \\boldsymbol \\beta)}{\\partial \\boldsymbol \\beta}  = \\boldsymbol a \\\\\n(ii)& \\quad &&\\frac{\\partial ( \\boldsymbol \\beta' \\boldsymbol A \\boldsymbol \\beta)}{\\partial \\boldsymbol \\beta}   = (\\boldsymbol A + \\boldsymbol A') \\boldsymbol \\beta.\n\\end{align*}\\]"
  },
  {
    "objectID": "05_calculus.html#hessian",
    "href": "05_calculus.html#hessian",
    "title": "5  Matrix calculus",
    "section": "5.2 Hessian",
    "text": "5.2 Hessian\nThe second derivatives matrix or Hessian is the \\(k \\times k\\) matrix \\[\n    \\frac{\\partial^2 f(\\boldsymbol \\beta)}{\\partial\\boldsymbol{\\beta }\\partial \\boldsymbol{\\beta}'}\n    =  \\begin{pmatrix}\\frac{\\partial^2 f(\\boldsymbol \\beta)}{\\partial \\beta_1 \\partial \\beta_1} & \\ldots & \\frac{\\partial^2 f(\\boldsymbol \\beta)}{\\partial \\beta_k \\partial \\beta_1} \\\\\n    \\vdots & & \\vdots \\\\\n    \\frac{\\partial^2 f(\\boldsymbol \\beta)}{\\partial \\beta_1 \\partial \\beta_k} & \\ldots & \\frac{\\partial^2 f(\\boldsymbol \\beta)}{\\partial \\beta_k \\partial \\beta_k}\n    \\end{pmatrix}.\\]\nIf the Hessian is evaluated at some particular value \\(\\boldsymbol \\beta = \\boldsymbol b\\), we write \\[\n\\frac{\\partial^2 f}{\\partial\\boldsymbol{\\beta }\\partial \\boldsymbol{\\beta}'}(\\boldsymbol b)\n\\]\nThe Hessian is symmetric. Each column of the Hessian is the derivative of the components of the gradient for the corresponding variable in \\(\\boldsymbol \\beta'\\):\n\\[\\begin{align*}\n\\frac{\\partial^2 f(\\boldsymbol \\beta)}{\\partial\\boldsymbol{\\beta }\\partial \\boldsymbol{\\beta}'}\n    &= \\frac{\\partial(\\partial f(\\boldsymbol \\beta)/\\partial \\boldsymbol \\beta)}{\\partial \\boldsymbol \\beta'} \\\\\n    &= \\Bigg[ \\frac{\\partial(\\partial f(\\boldsymbol \\beta)/\\partial \\boldsymbol \\beta)}{\\partial \\beta_1} \\ \\frac{\\partial(\\partial f(\\boldsymbol \\beta)/\\partial \\boldsymbol \\beta)}{\\partial \\beta_2} \\ \\ldots \\ \\frac{\\partial(\\partial f(\\boldsymbol \\beta)/\\partial \\boldsymbol \\beta)}{\\partial \\beta_n} \\Bigg]\n\\end{align*}\\]\nThe Hessian of a sandwich form function is \\[\n  \\frac{\\partial^2 ( \\boldsymbol \\beta' \\boldsymbol A \\boldsymbol \\beta)}{\\partial \\boldsymbol \\beta \\partial \\boldsymbol \\beta'}  = \\boldsymbol A + \\boldsymbol A'.\n\\]"
  },
  {
    "objectID": "05_calculus.html#optimization",
    "href": "05_calculus.html#optimization",
    "title": "5  Matrix calculus",
    "section": "5.3 Optimization",
    "text": "5.3 Optimization\nRecall the first-order (necessary) and second-order (sufficient) conditions for optimum (maximum or minimum) in the univariate case:\n\nFirst-order condition: the first derivative evaluated at the optimum is zero.\nSecond-order condition: the second derivative at the optimum is negative for a maximum and positive for a minimum.\n\nSimilarly, we formulate first and second-order conditions for a function \\(f(\\boldsymbol \\beta)\\). The first-order condition for an optimum (maximum or minimum) at \\(\\boldsymbol b\\) is \\[\n  \\frac{\\partial f}{\\partial\\boldsymbol{\\beta}}(\\boldsymbol b)  = \\boldsymbol 0.\n\\] The second-order condition is \\[\\begin{align*}\n  &\\frac{\\partial^2 f}{\\partial\\boldsymbol{\\beta }\\partial \\boldsymbol{\\beta}'}(\\boldsymbol b) &gt; 0 \\quad \\text{for a minimum at} \\ \\boldsymbol b, \\\\\n  &\\frac{\\partial^2 f}{\\partial\\boldsymbol{\\beta }\\partial \\boldsymbol{\\beta}'}(\\boldsymbol b) &lt; 0 \\quad \\text{for a maximum at} \\ \\boldsymbol b.\n\\end{align*}\\] Recall that, in the context of matrices, the notation “\\(&gt; 0\\)” means positive definite, and “\\(&lt; 0\\)” means negative definite."
  },
  {
    "objectID": "06_exercises.html#solutions",
    "href": "06_exercises.html#solutions",
    "title": "6  Problems",
    "section": "6.1 Solutions",
    "text": "6.1 Solutions\nSolutions to the problems are available here (unfortunately only in German so far)"
  }
]